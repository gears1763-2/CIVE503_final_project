\documentclass[12pt, letterpaper]{report}

% Package uses
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{verbatim}
\usepackage[all]{nowidow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
    \theoremstyle{definition}
    \newtheorem{assumption}{Assumption}
    \newtheorem{corollary}{Corollary}
    \newtheorem{definition}{Definition}
    \newtheorem{theorem}{Theorem}
    \newtheorem{proposition}{Proposition}
\usepackage{amssymb}
%\usepackage[dvipsnames]{xcolor}
\usepackage{cite}
%\usepackage{fancyvrb}
\usepackage{float}
\usepackage{graphicx}
	\graphicspath{ {figures} }
%\usepackage{lscape}
%\usepackage{caption}
%\usepackage{subcaption}
\usepackage[margin = 1in]{geometry}
%\usepackage{pythonhighlight}
%\usepackage{minted}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{titlesec}
\usepackage{lineno}

\frenchspacing

\renewcommand{\bibname}{References}

\renewcommand{\chaptername}{}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{1em}{} 

\begin{document}

%%%% ==== TITLE PAGE ==== %%%%

%\begin{titlepage}
%	\vspace*{\stretch{0.5}}
%	\begin{center}
%	  	 \LARGE \textbf{MECH 542 - Energy Systems} \\
%	 	 \vspace{8mm}
%        \LARGE \textbf{Assignment 1} \\
%	 	 \vfill
%	 	 \normalsize\textit{prepared by} \\
%	 	 \vspace{10mm}
%	 	 \large Anthony Truelove \footnotesize MASc, P.Eng. \\
%        \vspace{3mm}
%        \normalsize PhD Student (V00189468) / Research Engineer (PRIMED) \\
%        \vspace{30mm}
%        \normalsize\textit{prepared for} \\
%        \vspace{10mm}
%        \large Dr. Andrew Rowe \\
%        \vspace{4mm}
%        \normalsize Professor - Mechanical Engineering (UVic) \\
%	\end{center}
%	\vfill
%	\begin{center}
%		\today \\
%		\vspace{3mm}
%		Drafted using \LaTeX
%	\end{center}	   
%\end{titlepage}


%%%% ==== MAIN ==== %%%%

\section*{Motivation}

Consider a black-box function $f(\vec{x})$ of the form

\begin{equation}
	\begin{matrix}
		f\;:\;\mathcal{S}\subseteq\mathbb{R}^D\to\mathbb{R} \\
		\vec{x}\;\mapsto\;f(\vec{x})
	\end{matrix}
	\label{eqn:f}
\end{equation}

\noindent and assume that, while evaluating $f(\vec{x})$ is always possible,

\begin{enumerate}
	\item $f$ does not have an explicit, closed form expression; and,
	\item $f$ is expensive to compute.
\end{enumerate}

\noindent If the goal is to seek $\vec{x}^*\in\mathcal{S}$ such that $f$ is minimized,\footnote{So $f(\vec{x}^*)\leq f(\vec{x})$ for all $\vec{x}\in\mathcal{S}$.} then this motivates finding a surrogate function $\widehat{f}(\vec{x})$ such that

\begin{enumerate}
	\item $\widehat{f}(\vec{x})\cong f(\vec{x})$; and,
	\item $\widehat{f}$ is cheap to compute.
\end{enumerate}

\noindent This then allows one to perform surrogate optimization (i.e., using $\widehat{f}$ as the objective function within some optimization algorithm in order to approximate the optima of $f$).

\section*{Question}

The construction of any surrogate model involves first sampling the problem space. That is, choose a set of $N>0$ sample points $\left\{\vec{s}_i\right\}$, generate the corresponding image set $\left\{f(\vec{s}_i)\right\}$, and then train a surrogate model using this data set. This then begs the question: \textcolor{red}{\textit{what is the most efficient sampling scheme?}}\par

\section*{Methodology}

For the sake of scope control, this work is focussed on scalar functions $f$ as stated above. The idea here is to compare two sampling schemes; namely

\begin{enumerate}
	\item Simple random sampling (the ``null sampling"); and,
	\item Latin hypercube sampling (the ``alternate sampling").
\end{enumerate} 

\noindent For a given benchmark problem (\textcolor{red}{I'll just pick one, for the sake of scope.}), the \textit{efficiency} of a sampling scheme might be defined as something like

\begin{equation}
	\eta_s = \frac{\textrm{surrogate utility}}{\textrm{surrogate cost}} = \exp\left[-\frac{N(\mu + \sigma)}{M}\right]
	\label{eqn:sampling_efficiency_concept}
\end{equation}
\noindent where $\mu\geq 0$ is some performance (error) mean, $\sigma\geq 0$ is some performance (error) standard deviation, and $M>0$ is some normalizing factor (\textcolor{red}{perhaps just $M = \textrm{max}\left\{|f(\vec{s}_i)|\right\}$?}). Observe that (\ref{eqn:sampling_efficiency_concept}) is defined such that

\begin{enumerate}
	\item For any $\mu, \sigma, M > 0$, $\eta_s\to 0^+$ as $N\to\infty$. So, efficiency tends to zero for larger and larger sample sizes (the cost penalty).
	\item For any $N,M>0$, $\eta_s\to 1^-$ as $\mu + \sigma \to 0$. So, efficiency tends to one in the case of ``perfect surrogate utility" (the utility reward).
\end{enumerate}

\noindent What remains, then, is defining and computing $\mu$ and $\sigma$.\par 
As for $\mu$ and $\sigma$, one might choose $\mu = \mu_\textrm{RMSE}$ and $\sigma = \sigma_\textrm{RMSE}$ (that is, work in terms of surrogate root mean squared error). One way forward in this regard is a training/test split approach; namely

\begin{enumerate}
	\item Randomly partition the surrogate data set into two sub-sets: training and test. (\textcolor{red}{Perhaps an 80\%/20\% split here.})
	\item Train the surrogate on the training set (e.g., least squares).
	\item Assess the surrogate on the test set (compute RMSE).
	\item Repeat 1 - 3 a sufficient number of times (say, 1000) to generate a population of RMSE values. (\textcolor{red}{So essentially \textit{bootstrapping}.})
	\item Compute $\mu_\textrm{RMSE}$ and $\sigma_\textrm{RMSE}$ from the population generated in 4.
\end{enumerate}

\noindent Steps 1 - 5 would allow one to generate a single $(N, \eta_s)$ pair under some choice of dimensionality $D$ and sampling scheme. \textcolor{red}{\textit{One of the key deliverables here would be $\eta_s$ vs $N$ curves for various $D$ values, one plot for each sampling scheme (or maybe a single plot with dashed curves for random sampling and solid curves for LHS). I expect the ``curse of dimensionality" will present in the plot(s).}}\par 
\vspace{4mm}
\textcolor{red}{\textit{Taking this one step further, if I were to vary the benchmark problem as well, then I could gain some insight into whether or not the No Free Lunch Theorem also holds with respect to sampling scheme (i.e., there is no overall ``best scheme"). I suspect that it \underline{does not}, actually, but that remains to be seen.}}


%%%% ==== REFERENCES ==== %%%%

%\newpage
%\bibliography{/home/primed-anthony/MECH_PhD/tex/refs/refs.bib}{}
%\bibliographystyle{IEEEtran} 
%\addcontentsline{toc}{chapter}{References}

\end{document}


%%%% ==== TEMPLATES ==== %%%%


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{spec_rad_legacy.png}
    \caption{$\rho(\Phi)$ for varying $\frac{\Delta t}{T_n}$ (in this case, $A=\Phi$ and $T_p=T_n$)}
    \label{fig:spec_rad_legacy}
\end{figure}


\begin{table}[H]
    \centering
    \caption{Table of $P_{\underline{H}_s}(H_s)$ vs $H_s$}
    \begin{tabular}{|c|c|}
        \hline
        $P_{\underline{H}_s}(H_s)$ & $H_s$ \\\hline\hline
        $P_1$ & $H_{s1}$ \\\hline
        $P_2$ & $H_{s2}$ \\\hline
        $\vdots$ & $\vdots$ \\\hline
        $P_n$ & $H_{sn}$ \\\hline 
    \end{tabular}
    \label{tab:H_vs_P}
\end{table}


\begin{center}
\fbox{
\parbox{0.95\textwidth}{
\begin{assumption}{ \underline{Power Matters}:}
    With respect to $\frac{d\textrm{SOH}}{dt}$, the charge/discharge power of the battery matters. Therefore, if $P(t)\in[-P_\textrm{cap},P_\textrm{cap}]$ is charge/discharge power at time $t$, and $P_\textrm{cap}>0$ is charge/discharge power capacity (constant), then the dimensionless term
    $$ \frac{P(t)}{P_\textrm{cap}} $$
    
    is an input of $f$.
    \label{asm:power}
\end{assumption}
}}
\end{center}
